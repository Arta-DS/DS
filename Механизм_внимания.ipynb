{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzG0hQvaEwxtGxq5PhFEX6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arta-DS/DS/blob/main/%D0%9C%D0%B5%D1%85%D0%B0%D0%BD%D0%B8%D0%B7%D0%BC_%D0%B2%D0%BD%D0%B8%D0%BC%D0%B0%D0%BD%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxmiEffaQ3fV",
        "outputId": "71911ccb-1f48-4dd3-8c10-cd0799caadce"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-10-16 20:54:47--  http://www.manythings.org/anki/rus-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17201311 (16M) [application/zip]\n",
            "Saving to: ‘rus-eng.zip.1’\n",
            "\n",
            "rus-eng.zip.1       100%[===================>]  16.40M  23.9MB/s    in 0.7s    \n",
            "\n",
            "2025-10-16 20:54:48 (23.9 MB/s) - ‘rus-eng.zip.1’ saved [17201311/17201311]\n",
            "\n",
            "Archive:  rus-eng.zip\n",
            "replace rus.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "# Загрузка и распаковка данных\n",
        "!wget http://www.manythings.org/anki/rus-eng.zip\n",
        "!unzip rus-eng.zip\n",
        "\n",
        "# Чтение данных\n",
        "with open('rus.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "# Создание пар предложений\n",
        "pairs = [[line.split('\\t')[0], line.split('\\t')[1]] for line in lines if line]\n",
        "print(f\"Всего пар предложений: {len(pairs)}\")\n",
        "print(\"Пример пары:\", pairs[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка seed для воспроизводимости\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Инициализация токенизатора для английского\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Простая токенизация для русского языка (разделение по пробелам)\n",
        "def tokenize_ru(text):\n",
        "    return text.split()\n",
        "\n",
        "# Создание словарей\n",
        "class Vocabulary:\n",
        "    def __init__(self, tokenize_func):\n",
        "        self.tokenize = tokenize_func\n",
        "        self.word2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
        "        self.word_count = {}\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        for sentence in sentences:\n",
        "            for word in self.tokenize(sentence.lower()):\n",
        "                self.word_count[word] = self.word_count.get(word, 0) + 1\n",
        "        for word, count in self.word_count.items():\n",
        "            if count >= 2: # Добавляем слова, которые встретились хотя бы 2 раза\n",
        "                if word not in self.word2idx:\n",
        "                    self.word2idx[word] = len(self.word2idx)\n",
        "                    self.idx2word[len(self.idx2word)] = word\n",
        "\n",
        "    def numericalize(self, sentence):\n",
        "        tokens = self.tokenize(sentence.lower())\n",
        "        return [self.word2idx.get(token, self.word2idx['<unk>']) for token in tokens]\n",
        "\n",
        "# Подготовка данных\n",
        "en_sentences = [pair[0] for pair in pairs]\n",
        "ru_sentences = [pair[1] for pair in pairs]\n",
        "\n",
        "# Создание словарей\n",
        "SRC_vocab = Vocabulary(spacy_en.tokenizer)\n",
        "SRC_vocab.build_vocab(en_sentences)\n",
        "\n",
        "TRG_vocab = Vocabulary(tokenize_ru)\n",
        "TRG_vocab.build_vocab(ru_sentences)\n",
        "\n",
        "print(f\"Размер словаря английского языка (источник): {len(SRC_vocab.word2idx)}\")\n",
        "print(f\"Размер словаря русского языка (цель): {len(TRG_vocab.word2idx)}\")\n",
        "\n",
        "# Преобразование предложений в тензоры\n",
        "def process_data(pairs, src_vocab, trg_vocab):\n",
        "    src_data = []\n",
        "    trg_data = []\n",
        "    for src, trg in pairs:\n",
        "        src_indices = [src_vocab.word2idx['<sos>']] + src_vocab.numericalize(src) + [src_vocab.word2idx['<eos>']]\n",
        "        trg_indices = [trg_vocab.word2idx['<sos>']] + trg_vocab.numericalize(trg) + [trg_vocab.word2idx['<eos>']]\n",
        "        src_data.append(torch.tensor(src_indices, dtype=torch.long))\n",
        "        trg_data.append(torch.tensor(trg_indices, dtype=torch.long))\n",
        "    return src_data, trg_data\n",
        "\n",
        "src_data, trg_data = process_data(pairs, SRC_vocab, TRG_vocab)\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "train_size = int(0.9 * len(src_data))\n",
        "test_size = len(src_data) - train_size\n",
        "src_train, src_test = torch.utils.data.random_split(src_data, [train_size, test_size])\n",
        "trg_train, trg_test = torch.utils.data.random_split(trg_data, [train_size, test_size])\n",
        "\n",
        "# Создание DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "    src_batch = pad_sequence(src_batch, padding_value=SRC_vocab.word2idx['<pad>'])\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=TRG_vocab.word2idx['<pad>'])\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "train_dataset = list(zip(src_train, trg_train))\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "test_dataset = list(zip(src_test, trg_test))\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Обучающая выборка: {len(train_dataset)} пар\")\n",
        "print(f\"Тестовая выборка: {len(test_dataset)} пар\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3hCqHI2Q-zy",
        "outputId": "52e735f7-2ba5-4e74-a6b9-9fe106cbb2cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер словаря английского языка (источник): 4\n",
            "Размер словаря русского языка (цель): 59022\n",
            "Обучающая выборка: 474877 пар\n",
            "Тестовая выборка: 52765 пар\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src = [src_len, batch_size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded = [src_len, batch_size, emb_dim]\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        # outputs = [src_len, batch_size, hid_dim * num_directions]\n",
        "        # hidden = [n_layers * num_directions, batch_size, hid_dim]\n",
        "\n",
        "        # Инициализация состояния декодера\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        # hidden = [batch_size, dec_hid_dim]\n",
        "\n",
        "        return outputs, hidden"
      ],
      "metadata": {
        "id": "WeQKhfkiQ_Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        # hidden = [batch_size, dec_hid_dim]\n",
        "        # encoder_outputs = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        # mask = [batch_size, src_len]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        # Повторяем скрытое состояние декодера для каждого слова в источнике\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        # hidden = [batch_size, src_len, dec_hid_dim]\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        # energy = [batch_size, src_len, dec_hid_dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        # attention = [batch_size, src_len]\n",
        "\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        return torch.softmax(attention, dim=1)"
      ],
      "metadata": {
        "id": "yyAacWmLRC54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(enc_hid_dim * 2 + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        # hidden = [batch_size, dec_hid_dim]\n",
        "        # encoder_outputs = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        # mask = [batch_size, src_len]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        # Повторяем скрытое состояние декодера для каждого слова в источнике\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        # hidden = [batch_size, src_len, dec_hid_dim]\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
        "\n",
        "        # Конкатенируем скрытые состояния энкодера и декодера\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        # energy = [batch_size, src_len, dec_hid_dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        # attention = [batch_size, src_len]\n",
        "\n",
        "        # Применяем маску, чтобы модель не обращала внимание на <pad> токены\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        return torch.softmax(attention, dim=1)"
      ],
      "metadata": {
        "id": "PHyajpgTRFm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "        # input = [batch_size]\n",
        "        # hidden = [batch_size, dec_hid_dim]\n",
        "        # encoder_outputs = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        # mask = [batch_size, src_len]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        # input = [1, batch_size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded = [1, batch_size, emb_dim]\n",
        "\n",
        "        # Вычисление весов внимания\n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "        # a = [batch_size, src_len]\n",
        "\n",
        "        a = a.unsqueeze(1)\n",
        "        # a = [batch_size, 1, src_len]\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
        "\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        # weighted = [batch_size, 1, enc_hid_dim * 2]\n",
        "\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        # weighted = [1, batch_size, enc_hid_dim * 2]\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        # rnn_input = [1, batch_size, (enc_hid_dim * 2) + emb_dim]\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        # output = [seq_len, batch_size, dec_hid_dim]\n",
        "        # hidden = [n_layers, batch_size, dec_hid_dim]\n",
        "\n",
        "        assert (output == hidden).all()\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        # prediction = [batch_size, output_dim]\n",
        "\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "metadata": {
        "id": "h-QE_jQvRK7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        # src = [src_len, batch_size]\n",
        "        # trg = [trg_len, batch_size]\n",
        "\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Тензор для хранения выходов\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Скрытое состояние энкодера\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        # encoder_outputs = [src_len, batch_size, enc_hid_dim * 2]\n",
        "        # hidden = [batch_size, dec_hid_dim]\n",
        "\n",
        "        # Первый вход в декодер - это <sos> токен\n",
        "        input = trg[0,:]\n",
        "\n",
        "        mask = self.create_mask(src)\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            # Вставляем входное слово, предыдущее скрытое состояние и все выходы энкодера\n",
        "            # Получаем предсказание, новое скрытое состояние и веса внимания\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            # output = [batch_size, output_dim]\n",
        "            # hidden = [batch_size, dec_hid_dim]\n",
        "\n",
        "            # Сохраняем предсказание\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Решаем, использовать ли teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "5hdMrCPORN4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(SRC_vocab.word2idx)\n",
        "OUTPUT_DIM = len(TRG_vocab.word2idx)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC_vocab.word2idx['<pad>']\n",
        "\n",
        "# --- Модель 1: Внимание на основе скалярного произведения ---\n",
        "attn_dot = DotProductAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc_dot = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec_dot = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn_dot)\n",
        "model_dot = Seq2Seq(enc_dot, dec_dot, SRC_PAD_IDX, DEVICE).to(DEVICE)\n",
        "\n",
        "# --- Модель 2: Внимание на основе MLP ---\n",
        "attn_mlp = MLPAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc_mlp = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec_mlp = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn_mlp)\n",
        "model_mlp = Seq2Seq(enc_mlp, dec_mlp, SRC_PAD_IDX, DEVICE).to(DEVICE)\n",
        "\n",
        "\n",
        "def initialize_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "model_dot.apply(initialize_weights)\n",
        "model_mlp.apply(initialize_weights)\n",
        "\n",
        "# Оптимизаторы и функция потерь\n",
        "optimizer_dot = optim.Adam(model_dot.parameters())\n",
        "optimizer_mlp = optim.Adam(model_mlp.parameters())\n",
        "\n",
        "TRG_PAD_IDX = TRG_vocab.word2idx['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "metadata": {
        "id": "YrP0JXotRQzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        # output = [trg_len, batch_size, output_dim]\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        # trg = [(trg_len - 1) * batch_size]\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (src, trg) in enumerate(iterator):\n",
        "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "\n",
        "            output = model(src, trg, 0) # turn off teacher forcing\n",
        "            # output = [trg_len, batch_size, output_dim]\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "            # trg = [(trg_len - 1) * batch_size]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "GCWlfCmpRWDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "print(\"--- Обучение модели с Dot-Product Attention ---\")\n",
        "best_valid_loss_dot = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model_dot, train_loader, optimizer_dot, criterion, CLIP)\n",
        "    valid_loss = evaluate(model_dot, test_loader, criterion)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss_dot:\n",
        "        best_valid_loss_dot = valid_loss\n",
        "        torch.save(model_dot.state_dict(), 'dot-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "print(\"\\n--- Обучение модели с MLP Attention ---\")\n",
        "best_valid_loss_mlp = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model_mlp, train_loader, optimizer_mlp, criterion, CLIP)\n",
        "    valid_loss = evaluate(model_mlp, test_loader, criterion)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss_mlp:\n",
        "        best_valid_loss_mlp = valid_loss\n",
        "        torch.save(model_mlp.state_dict(), 'mlp-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb7swP0vRY4_",
        "outputId": "a57be739-b687-4478-b190-c77e5966754e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Обучение модели с Dot-Product Attention ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка лучших весов\n",
        "model_dot.load_state_dict(torch.load('dot-model.pt'))\n",
        "model_mlp.load_state_dict(torch.load('mlp-model.pt'))\n",
        "\n",
        "def translate_sentence(sentence, model, src_vocab, trg_vocab, device, max_len=50):\n",
        "    model.eval()\n",
        "    tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
        "    tokens = [src_vocab.word2idx.get(token, src_vocab.word2idx['<unk>']) for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
        "    src_len = torch.LongTensor([len(tokens)]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "\n",
        "    trg_indexes = [trg_vocab.word2idx['<sos>']]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, _ = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_vocab.word2idx['<eos>']:\n",
        "            break\n",
        "\n",
        "    trg_tokens = [trg_vocab.idx2word[i] for i in trg_indexes]\n",
        "    return trg_tokens[1:] # Убираем <sos>"
      ],
      "metadata": {
        "id": "qSsbmgmyRbBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = random.randint(0, len(test_dataset))\n",
        "src_sentence = src_test.dataset[example_idx].tolist()\n",
        "trg_sentence = trg_test.dataset[example_idx].tolist()\n",
        "\n",
        "# Преобразуем обратно в текст\n",
        "src_text = ' '.join([SRC_vocab.idx2word[idx] for idx in src_sentence if idx not in [SRC_vocab.word2idx['<sos>'], SRC_vocab.word2idx['<eos>'], SRC_vocab.word2idx['<pad>']]])\n",
        "trg_text = ' '.join([TRG_vocab.idx2word[idx] for idx in trg_sentence if idx not in [TRG_vocab.word2idx['<sos>'], TRG_vocab.word2idx['<eos>'], TRG_vocab.idx2word['<pad>']]])\n",
        "\n",
        "print(f'Исходное предложение (EN): {src_text}')\n",
        "print(f'Эталонный перевод (RU): {trg_text}\\n')\n",
        "\n",
        "# Перевод моделью с Dot-Product Attention\n",
        "translation_dot = translate_sentence(src_text, model_dot, SRC_vocab, TRG_vocab, DEVICE)\n",
        "print(f'Перевод (Dot-Product Attention): {\" \".join(translation_dot)}')\n",
        "\n",
        "# Перевод моделью с MLP Attention\n",
        "translation_mlp = translate_sentence(src_text, model_mlp, SRC_vocab, TRG_vocab, DEVICE)\n",
        "print(f'Перевод (MLP Attention): {\" \".join(translation_mlp)}')"
      ],
      "metadata": {
        "id": "B_VQuOABRbd_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
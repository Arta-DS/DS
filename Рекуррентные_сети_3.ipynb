{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfEzzcO0CXWFkdSw2V8kRK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arta-DS/DS/blob/main/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возьмите англо-русскую пару фраз (https://www.manythings.org/anki/)\n",
        "\n",
        "1.   Обучите на них seq2seq по аналогии с занятием. Оцените полученное качество\n",
        "2.   Попробуйте добавить +1 рекуррентный слой в encoder и decoder\n",
        "3.   Попробуйте заменить GRU ячейки на lstm-ячейки\n",
        "4.   Оцените качество во всех случаях"
      ],
      "metadata": {
        "id": "zmZvEg_-4wPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJOAHjxppQPk",
        "outputId": "364869d1-d1bb-4b71-dc27-a20526e0a25a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Jq-vqmqbsq",
        "outputId": "77d4dd0b-3386-43f8-c8d4-3a8d341ba3a2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IGFTuZxu4E3K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import spacy\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Подготовка данных\n",
        "local_zip_path = '/content/rus-eng.zip'\n",
        "file = None # Инициализируем переменную\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(local_zip_path, 'r') as z:\n",
        "        with z.open('rus.txt') as f:\n",
        "            file_content = f.read().decode('utf-8')\n",
        "            file = file_content.splitlines()\n",
        "    print(f\"Файл успешно загружен и распакован локально. Количество строк: {len(file)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ОШИБКА: Файл не найден по пути '{local_zip_path}'. Убедитесь, что вы скачали его и положили в нужную папку.\")\n",
        "    # Прерываем выполнение, если файл не найден\n",
        "    raise SystemExit(\"Остановка выполнения: файл с данными не найден.\")\n",
        "except Exception as e:\n",
        "    print(f\"Произошла ошибка при распаковке: {e}\")\n",
        "    raise SystemExit(\"Остановка выполнения: не удалось распаковать файл.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2DYPQVXCIzu",
        "outputId": "7168c60d-a4e7-4adb-a7e0-b702e3881f23"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл успешно загружен и распакован локально. Количество строк: 527642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  2. Предобработка и токенизация\n",
        "\n",
        "try:\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "except OSError:\n",
        "    print(\"Модель spacy для английского не найдена. Используем простую токенизацию.\")\n",
        "    def tokenize_en(text):\n",
        "        return text.split(' ')\n",
        "\n",
        "def tokenize_ru(text):\n",
        "    text = text.replace('.', ' .').replace(',', ' ,').replace('!', ' !').replace('?', ' ?')\n",
        "    return text.split(' ')"
      ],
      "metadata": {
        "id": "wcx3zWhK63ht"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  3. Создание словарей\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "UNK_token = 3\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"<SOS>\": SOS_token, \"<EOS>\": EOS_token, \"<PAD>\": PAD_token, \"<UNK>\": UNK_token}\n",
        "        self.index2word = {v: k for k, v in self.word2index.items()}\n",
        "        self.word2count = {}\n",
        "        self.n_words = len(self.word2index) # Считаем все служебные токены\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in tokenize(self.name, sentence):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "def tokenize(lang, sentence):\n",
        "    if lang == 'en':\n",
        "        return tokenize_en(sentence)\n",
        "    else:\n",
        "        return tokenize_ru(sentence)\n",
        "\n",
        "MAX_LEN = 10\n",
        "pairs = []\n",
        "eng = Lang('eng')\n",
        "rus = Lang('rus')\n",
        "\n",
        "print(\"Подготовка пар и словарей...\")\n",
        "for line in tqdm(file):\n",
        "    parts = line.strip().split('\\t')\n",
        "    if len(parts) < 2: continue\n",
        "    en_sentence, ru_sentence = parts[0], parts[1]\n",
        "\n",
        "    if len(tokenize('en', en_sentence)) > MAX_LEN or len(tokenize('ru', ru_sentence)) > MAX_LEN:\n",
        "        continue\n",
        "\n",
        "    eng.addSentence(en_sentence)\n",
        "    rus.addSentence(ru_sentence)\n",
        "    pairs.append((en_sentence, ru_sentence))\n",
        "\n",
        "print(f\"Всего пар: {len(pairs)}\")\n",
        "print(f\"Размер англ. словаря: {eng.n_words}\")\n",
        "print(f\"Размер рус. словаря: {rus.n_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXgza6Fz68Ac",
        "outputId": "daec4951-8352-4fc7-8361-48d1f8a9474d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Подготовка пар и словарей...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 527642/527642 [00:18<00:00, 28732.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего пар: 464716\n",
            "Размер англ. словаря: 19720\n",
            "Размер рус. словаря: 62689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  4. Dataset и DataLoader\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, src_lang, trg_lang):\n",
        "        self.pairs = pairs\n",
        "        self.src_lang = src_lang\n",
        "        self.trg_lang = trg_lang\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_sentence, trg_sentence = self.pairs[idx]\n",
        "        src_indices = [self.src_lang.word2index.get(word, UNK_token) for word in tokenize(self.src_lang.name, src_sentence)]\n",
        "        trg_indices = [self.trg_lang.word2index.get(word, UNK_token) for word in tokenize(self.trg_lang.name, trg_sentence)]\n",
        "\n",
        "        src_indices.append(EOS_token)\n",
        "        trg_indices.append(EOS_token)\n",
        "\n",
        "        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(trg_indices, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=PAD_token, batch_first=True)\n",
        "    trg_batch = nn.utils.rnn.pad_sequence(trg_batch, padding_value=PAD_token, batch_first=True)\n",
        "    return src_batch, trg_batch"
      ],
      "metadata": {
        "id": "0DNT2vcP6_FM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  5. Модель Seq2Seq (гибкая архитектура)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, rnn_type):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=True)\n",
        "        else: # GRU\n",
        "            self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        return hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, rnn_type):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=True)\n",
        "        else: # GRU\n",
        "            self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, batch_first=True)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        hidden = self.encoder(src)\n",
        "\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            prediction, hidden = self.decoder(input, hidden)\n",
        "            outputs[:, t] = prediction\n",
        "\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = prediction.argmax(1)\n",
        "\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "yNEmVvhX7H6T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  6. Функции для обучения и перевода\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, (src, trg) in enumerate(tqdm(iterator, desc=\"Training\")):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def translate_sentence(sentence, src_lang, trg_lang, model, device, max_len=50):\n",
        "    model.eval()\n",
        "    tokens = [token.lower() for token in tokenize(src_lang.name, sentence)]\n",
        "    tokens = [src_lang.word2index.get(tok, UNK_token) for tok in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(src_tensor)\n",
        "\n",
        "    trg_indexes = [SOS_token]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(trg_tensor, hidden)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == EOS_token:\n",
        "            break\n",
        "\n",
        "    trg_tokens = [trg_lang.index2word[i] for i in trg_indexes]\n",
        "\n",
        "    return trg_tokens[1:-1]"
      ],
      "metadata": {
        "id": "a-dhg4MoTbyX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  7. Запуск экспериментов (оптимизировано для CPU)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Используемое устройство: {device}\")\n",
        "\n",
        "# 1. Берем только часть данных для быстрого теста\n",
        "print(f\"Исходное количество пар: {len(pairs)}\")\n",
        "pairs_small = random.sample(pairs, 10000) # Берем только 10 000 пар\n",
        "print(f\"Новое количество пар для обучения: {len(pairs_small)}\")\n",
        "\n",
        "# 2. Уменьшаем размер модели\n",
        "INPUT_DIM = eng.n_words\n",
        "OUTPUT_DIM = rus.n_words\n",
        "ENC_EMB_DIM = 128  # Было 256\n",
        "DEC_EMB_DIM = 128  # Было 256\n",
        "HID_DIM = 256      # Было 512\n",
        "CLIP = 1\n",
        "N_EPOCHS = 2       # Было 10\n",
        "BATCH_SIZE = 128   # Увеличили для скорости\n",
        "\n",
        "dataset = TranslationDataset(pairs_small, eng, rus) # Используем урезанный датасет\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "def run_experiment(rnn_type, n_layers, exp_name):\n",
        "    print(f\"\\n{'='*20} Запуск эксперимента: {exp_name} {'='*20}\")\n",
        "\n",
        "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, n_layers, rnn_type)\n",
        "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, n_layers, rnn_type)\n",
        "    model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "        print(f'Эпоха: {epoch+1:02} | Потеря на обучении: {train_loss:.3f}')\n",
        "\n",
        "    print(\"\\nПримеры перевода:\")\n",
        "    for i in range(5):\n",
        "        src, trg = pairs[i] # Выводим примеры из полного набора\n",
        "        translation = translate_sentence(src, eng, rus, model, device)\n",
        "        original = tokenize('ru', trg)\n",
        "        print(f\"Оригинал (EN): {src}\")\n",
        "        print(f\"Эталон (RU):  {' '.join(original)}\")\n",
        "        print(f\"Перевод (RU):  {' '.join(translation)}\\n\")\n",
        "    return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cqFe4gpE070",
        "outputId": "095e111b-44db-4a69-ecd2-0ad5850a78a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используемое устройство: cuda\n",
            "Исходное количество пар: 464716\n",
            "Новое количество пар для обучения: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  7. Запуск экспериментов (Золотая середина)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Используемое устройство: {device}\")\n",
        "\n",
        "#  НАСТРОЙКИ \"ЗОЛОТОЙ СЕРЕДИНЫ\"\n",
        "# Компромисс между временем обучения и качеством модели\n",
        "NUM_SAMPLES = 100000\n",
        "print(f\"Исходное количество пар: {len(pairs)}\")\n",
        "pairs_medium = random.sample(pairs, NUM_SAMPLES)\n",
        "print(f\"Новое количество пар для обучения: {len(pairs_medium)}\")\n",
        "\n",
        "INPUT_DIM = eng.n_words\n",
        "OUTPUT_DIM = rus.n_words\n",
        "\n",
        "# Модель оставляем небольшой для скорости на CPU\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "HID_DIM = 256\n",
        "CLIP = 1\n",
        "N_EPOCHS = 5      # Увеличиваем количество эпох\n",
        "BATCH_SIZE = 128  # Оставляем большим для скорости\n",
        "\n",
        "dataset = TranslationDataset(pairs_medium, eng, rus)\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model_gru_1l_medium = run_experiment(rnn_type='GRU', n_layers=1, exp_name=\"GRU 1 слой (ЗОЛОТАЯ СЕРЕДИНА)\")\n",
        "\n",
        "print(\"\\nЭксперимент завершен!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np8jRSLocajk",
        "outputId": "13460d78-4e5e-43a9-dec0-66b077ea4cba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используемое устройство: cuda\n",
            "Исходное количество пар: 464716\n",
            "Новое количество пар для обучения: 100000\n",
            "\n",
            "==================== Запуск эксперимента: GRU 1 слой (ЗОЛОТАЯ СЕРЕДИНА) ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:52<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 01 | Потеря на обучении: 4.796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:53<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 02 | Потеря на обучении: 3.370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:54<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 03 | Потеря на обучении: 2.664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:54<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 04 | Потеря на обучении: 2.155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:55<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 05 | Потеря на обучении: 1.774\n",
            "\n",
            "Примеры перевода:\n",
            "Оригинал (EN): Go.\n",
            "Эталон (RU):  Марш !\n",
            "Перевод (RU):  идти .\n",
            "\n",
            "Оригинал (EN): Go.\n",
            "Эталон (RU):  Иди .\n",
            "Перевод (RU):  идти .\n",
            "\n",
            "Оригинал (EN): Go.\n",
            "Эталон (RU):  Идите .\n",
            "Перевод (RU):  идти .\n",
            "\n",
            "Оригинал (EN): Hi.\n",
            "Эталон (RU):  Здравствуйте .\n",
            "Перевод (RU):  с .\n",
            "\n",
            "Оригинал (EN): Hi.\n",
            "Эталон (RU):  Привет !\n",
            "Перевод (RU):  с .\n",
            "\n",
            "\n",
            "Эксперимент завершен!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы проведем два эксперимента с идентичными параметрами, но разной RNN-ячейкой, и оценим их с помощью стандартной метрики качества для машинного перевода — **BLEU**. Это метрика от 0 до 1 (или 0 до 100), которая сравнивает перевод, сгенерированный моделью, с одним или несколькими эталонными (человеческими) переводами. Чем больше совпадений слов и фраз, тем выше score.\n",
        "\n",
        "\n",
        "**Данные**: Мы разделим наши данные на обучающую и тестовую выборки. Модель будет учиться на одной части, а качество будем измерять на другой, которую она никогда не видела. Это даст объективную оценку."
      ],
      "metadata": {
        "id": "12n2uvCf5Q-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  1. Подготовка данных: Разделение на Train и Test\n",
        "\n",
        "# Используем 100к для обучения и 5к для теста\n",
        "NUM_TRAIN_SAMPLES = 100000\n",
        "NUM_TEST_SAMPLES = 5000\n",
        "\n",
        "# Перемешиваем все пары для случайного выбора\n",
        "random.shuffle(pairs)\n",
        "\n",
        "train_pairs = pairs[:NUM_TRAIN_SAMPLES]\n",
        "test_pairs = pairs[NUM_TRAIN_SAMPLES : NUM_TRAIN_SAMPLES + NUM_TEST_SAMPLES]\n",
        "\n",
        "print(f\"Размер обучающей выборки: {len(train_pairs)}\")\n",
        "print(f\"Размер тестовой выборки: {len(test_pairs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOAvKpGQn9t2",
        "outputId": "8cc4d53f-d4ba-44ce-99b3-b67fe2715cb1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер обучающей выборки: 100000\n",
            "Размер тестовой выборки: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  2. Функция для вычисления BLEU score\n",
        "\n",
        "def calculate_bleu(model, test_pairs, src_vocab, trg_vocab, device):\n",
        "    \"\"\"\n",
        "    Вычисляет BLEU score для модели на тестовой выборке.\n",
        "    \"\"\"\n",
        "    references, candidates = [], []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in test_pairs:\n",
        "            # Перевод от модели\n",
        "            translation = translate_sentence(src, src_vocab, trg_vocab, model, device)\n",
        "\n",
        "            # Эталонный перевод (список токенов)\n",
        "            reference = [tokenize('ru', trg)]\n",
        "\n",
        "            # Кандидат (перевод модели)\n",
        "            candidate = translation\n",
        "\n",
        "            references.append(reference)\n",
        "            candidates.append(candidate)\n",
        "\n",
        "    # corpus_bleu работает со списком предложений\n",
        "    # weights=(1, 0, 0, 0) смотрим только на точность совпадения отдельных слов (1-gram)\n",
        "    # Это более надежно для маленьких моделей, которые еще плохо строят фразы\n",
        "    bleu_score = corpus_bleu(references, candidates, weights=(1, 0, 0, 0))\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "gDweLw5moJ0m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment_eval(rnn_type, n_layers, exp_name, train_pairs, test_pairs):\n",
        "    print(f\"\\n{'='*20} Запуск эксперимента: {exp_name} {'='*20}\")\n",
        "\n",
        "    #  Параметры (одинаковые для обоих экспериментов)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    INPUT_DIM = eng.n_words\n",
        "    OUTPUT_DIM = rus.n_words\n",
        "    ENC_EMB_DIM = 128\n",
        "    DEC_EMB_DIM = 128\n",
        "    HID_DIM = 256\n",
        "    CLIP = 1\n",
        "    N_EPOCHS = 5\n",
        "    BATCH_SIZE = 128\n",
        "\n",
        "    #  Загрузчики данных\n",
        "    train_dataset = TranslationDataset(train_pairs, eng, rus)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    #  Инициализация модели\n",
        "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, n_layers, rnn_type)\n",
        "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, n_layers, rnn_type)\n",
        "    model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "\n",
        "    #  Обучение\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "        print(f'Эпоха: {epoch+1:02} | Потеря на обучении: {train_loss:.3f}')\n",
        "\n",
        "    #  Оценка качества\n",
        "    print(\"Вычисление BLEU score...\")\n",
        "    bleu = calculate_bleu(model, test_pairs, eng, rus, device)\n",
        "    print(f\"BLEU Score ({exp_name}): {bleu*100:.2f}\")\n",
        "\n",
        "    #  Примеры перевода\n",
        "    print(\"\\nПримеры перевода:\")\n",
        "    for i in range(3):\n",
        "        src, trg = test_pairs[i]\n",
        "        translation = translate_sentence(src, eng, rus, model, device)\n",
        "        original = tokenize('ru', trg)\n",
        "        print(f\"Оригинал (EN): {src}\")\n",
        "        print(f\"Эталон (RU):  {' '.join(original)}\")\n",
        "        print(f\"Перевод (RU):  {' '.join(translation)}\\n\")\n",
        "\n",
        "    return model, bleu"
      ],
      "metadata": {
        "id": "Q5ZJVSBioPCe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  4. Запуск сравнения\n",
        "\n",
        "# Эксперимент 1: GRU\n",
        "model_gru, bleu_gru = run_experiment_eval(\n",
        "    rnn_type='GRU',\n",
        "    n_layers=1,\n",
        "    exp_name=\"GRU 1 слой\",\n",
        "    train_pairs=train_pairs,\n",
        "    test_pairs=test_pairs\n",
        ")\n",
        "\n",
        "# Эксперимент 2: LSTM\n",
        "model_lstm, bleu_lstm = run_experiment_eval(\n",
        "    rnn_type='LSTM',\n",
        "    n_layers=1,\n",
        "    exp_name=\"LSTM 1 слой\",\n",
        "    train_pairs=train_pairs,\n",
        "    test_pairs=test_pairs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddmLvIXxoXX5",
        "outputId": "96af7a81-9fd5-455d-b926-c1c9d2c7637a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== Запуск эксперимента: GRU 1 слой ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:54<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 01 | Потеря на обучении: 4.812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:54<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 02 | Потеря на обучении: 3.392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:56<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 03 | Потеря на обучении: 2.671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:55<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 04 | Потеря на обучении: 2.171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:55<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 05 | Потеря на обучении: 1.788\n",
            "Вычисление BLEU score...\n",
            "BLEU Score (GRU 1 слой): 37.21\n",
            "\n",
            "Примеры перевода:\n",
            "Оригинал (EN): Tom bought a used truck.\n",
            "Эталон (RU):  Том купил подержанный грузовик .\n",
            "Перевод (RU):  купил купил подарок .\n",
            "\n",
            "Оригинал (EN): We know who you are.\n",
            "Эталон (RU):  Мы знаем , кто ты .\n",
            "Перевод (RU):  кто знал , кто ты такой .\n",
            "\n",
            "Оригинал (EN): I'd like to go with Tom.\n",
            "Эталон (RU):  Я хотел бы пойти с Томом .\n",
            "Перевод (RU):  хотел пойти с нами .\n",
            "\n",
            "\n",
            "==================== Запуск эксперимента: LSTM 1 слой ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:55<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 01 | Потеря на обучении: 5.016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:56<00:00,  2.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 02 | Потеря на обучении: 3.887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:58<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 03 | Потеря на обучении: 3.174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:59<00:00,  2.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 04 | Потеря на обучении: 2.637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 782/782 [04:56<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха: 05 | Потеря на обучении: 2.223\n",
            "Вычисление BLEU score...\n",
            "BLEU Score (LSTM 1 слой): 31.18\n",
            "\n",
            "Примеры перевода:\n",
            "Оригинал (EN): Tom bought a used truck.\n",
            "Эталон (RU):  Том купил подержанный грузовик .\n",
            "Перевод (RU):  купил билет .\n",
            "\n",
            "Оригинал (EN): We know who you are.\n",
            "Эталон (RU):  Мы знаем , кто ты .\n",
            "Перевод (RU):  знаю , кто ты .\n",
            "\n",
            "Оригинал (EN): I'd like to go with Tom.\n",
            "Эталон (RU):  Я хотел бы пойти с Томом .\n",
            "Перевод (RU):  с с с .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  5. Итоги сравнения\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ИТОГИ СРАВНЕНИЯ КАЧЕСТВА:\")\n",
        "print(f\"GRU BLEU Score: {bleu_gru*100:.2f}\")\n",
        "print(f\"LSTM BLEU Score: {bleu_lstm*100:.2f}\")\n",
        "if bleu_lstm > bleu_gru:\n",
        "    print(\"Победитель: LSTM (показал более высокое качество)\")\n",
        "elif bleu_gru > bleu_lstm:\n",
        "    print(\"Победитель: GRU (показал более высокое качество)\")\n",
        "else:\n",
        "    print(\"Ничья: модели показали одинаковое качество\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygcghJLqoawt",
        "outputId": "3d72c720-4418-4597-bf4b-922ee141d5eb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ИТОГИ СРАВНЕНИЯ КАЧЕСТВА:\n",
            "GRU BLEU Score: 37.21\n",
            "LSTM BLEU Score: 31.18\n",
            "Победитель: GRU (показал более высокое качество)\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Анализ результатов\n",
        "1. Потери на обучении (Training Loss)\n",
        "\n",
        "GRU: 1.788 (последняя эпоха)\n",
        "LSTM: 2.223 (последняя эпоха)\n",
        "Вывод: GRU не только обучился быстрее (его потери падали стремительнее), но и достиг меньшего финального значения потерь. Это говорит о том, что на наших данных GRU-модель смогла лучше \"подстроиться\" под обучающую выборку.\n",
        "\n",
        "2. BLEU Score (Объективное качество перевода)\n",
        "\n",
        "GRU: 37.21\n",
        "LSTM: 31.18\n",
        "Вывод: Это самый важный результат. Higher is better. GRU показал значительно более высокое качество перевода на тестовых данных, которые модель никогда не видела. Разница в 6 пунктов BLEU — это существенная разница, которая подтверждает, что GRU в данном случае не просто лучше запомнила данные, а научилась делать более качественные переводы.\n",
        "\n",
        "3. Примеры перевода\n",
        "\n",
        "Здесь мы видим подтверждение цифр:\n",
        "\n",
        "GRU делает более осмысленные, хоть и грамматически неверные, попытки. Например, хотел пойти с нами для I'd like to go with Tom — это довольно близко по смыслу.\n",
        "LSTM ведет себя более нестабильно. Он может дать отличный перевод (знаю , кто ты .), а может и полную бессмыслицу (с с с .). Такой разброс в качестве говорит о том, что модели сложнее стабильно выдавать хороший результат.\n",
        "Итоговый вывод\n",
        "Победитель: GRU.\n",
        "\n",
        "В нашем конкретном эксперименте, с заданными параметрами, объемом данных и временем обучения, GRU-ячейки оказались эффективнее LSTM-ячеек.\n",
        "\n",
        "Почему так произошло?\n",
        "\n",
        "Простота: У GRU меньше параметров (2 затвора против 3 у LSTM). На относительно простой задаче и не очень большой модели эта простота оказалась плюсом. Модели было легче и быстрее найти хорошие веса.\n",
        "Скорость обучения: GRU обучается быстрее, и за 5 эпох он успел \"научиться\" больше, чем LSTM.\n",
        "Переобучение: Более сложная LSTM-модель могла начать переобучаться или просто \"запутаться\" из-за своей сложности на ограниченном количестве данных и эпох.\n",
        "Это отличный пример того, что теоретическое преимущество LSTM в обработке длинных последовательностей не всегда превращается в практический выигрыш на конкретной задаче."
      ],
      "metadata": {
        "id": "McjNj7OM4ihp"
      }
    }
  ]
}